{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deterministic Processes\n",
    "Definition: A deterministic process is one in which no randomness is involved in the development of future states. Given an initial state, the subsequent states of the system are completely determined by the initial conditions and the rules governing the process.\n",
    "\n",
    "Stochastic Processes\n",
    "Definition: A stochastic process is one in which randomness is inherent in the development of future states. The process is influenced by random variables, making the outcome uncertain even if the initial conditions are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative models aim to learn the boundary between different classes in the data. They focus on modeling the conditional probability \n",
    "𝑃(𝑦∣𝑥), where y is the class label and x is the observed data.\n",
    "\n",
    "It is called discriminative because it is going to discriminate between the different types of input data.\n",
    "\n",
    "Estimate the probability of an image being a cat, given an image\n",
    "\n",
    "labels are provided, supervised learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative models aim to model the joint probability distribution P(x,y) of the observed data 𝑥 and the labels 𝑦. This means they learn how the data is generated, including the relationship between the features and the labels. Once they have learned this joint distribution, they can generate new data points by sampling from it.\n",
    "\n",
    "Generative models are for generating the probability of a certain piece of data. \n",
    "\n",
    "Estimating the probability of seeing a cat.\n",
    "\n",
    "No labels, self supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation learning\n",
    "\n",
    "Learn to form and group features of very high dimension (like images) in a space of less dimensions. \n",
    "instead of working with a higher dimension space directly, we describe the data using latent space of lower dimensionality.\n",
    "\n",
    "Reduce high dimensionality to a smaller dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to reduce the dimensions. Reduce high dimensional space to the lower dimension space.\n",
    "\n",
    "Then we are going to learn a mapping function that will take back the low dimension space to the higher dimension.\n",
    "Recreate images in the original domain that could be new cats, dogs or people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representative learning learns what are the most important features to describe the data and how to generate those characteristics.\n",
    "Advantage of working with representative learning is that it is easy to manipulate the space with lower dimensions. For example, stylegan can manipulate the degree of a smile in an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example, how do we increase the smile of an image ?\n",
    "\n",
    "For all images with a smile label, we will take average of positions in latent space. \n",
    "Then we will subtract that average from the mean of al images (not just smiling). \n",
    "That way we will obtain the vector that points from a non smile to a smile. \n",
    "\n",
    "Once we have that vector, we can increase the smile of an image by applying\n",
    "\n",
    "z = position in latent space.\n",
    "i = intensity of transformation\n",
    "\n",
    "z-new = z-old + i * vector-smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "morphing between images. \n",
    "\n",
    "p1 = position of image 1\n",
    "p2 = position of image 2\n",
    "\n",
    "alpha -interpolation between p1 and p2. \n",
    "\n",
    "p-new = p1 * alpha + (1-alpha) * p2\n",
    "\n",
    "p-new position in latent space through the line p1 to p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs have two parts\n",
    "\n",
    "Discriminator and Generator\n",
    "\n",
    "Generator is continuously trying to generate fake images and make it look as real as possible. It is trying to fool the discriminator. \n",
    "If discriminator is fooled, then it will get the loss value (difference between fake Y vs real label) (fake Y is generated from fake X)\n",
    "\n",
    "Discriminator job is to accurately guess the difference between a real image and an image generated by the generator. It also has a loss value. \n",
    "\n",
    "The generator and discriminator both have different objectives and hence have different loss values. \n",
    "\n",
    "\n",
    "Generator is trying to create fakes that look real.\n",
    "Discriminator is trying to differentiate between fake and reals. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL and Generative AI\n",
    "\n",
    "RL - Agents who learn to optimize a target in an environment through trial and error. \n",
    "RL + Generative AI = the agent hallucinates, generates, imagines scenarios where he tests instead of having to do them in simulation or real world.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real label is 1 - discriminator thinks image is real.  \n",
    "Fake label is 0 - discriminator thinks image is fake.  \n",
    "\n",
    "The generator weights should be pushed in a direction such that whatever image it gives to the discriminator it is detected as real by discriminator. \n",
    "That will be loss value 0. For loss value to be 0, discriminator should output 1 for image generated by generator ideally.\n",
    "\n",
    "But discriminator thinks it is 0.2 so we want to push the generator to slowly move the value to 1. \n",
    "1 output means that the discriminator thinks that image generated by the genereator is real. \n",
    "\n",
    "If it is 0, then discriminator thinks image is fake and generator is still not able to fool the discriminator.\n",
    "Then generator weights are adjusted using backpropagation based on loss value. \n",
    "\n",
    "That is how the generator weights are continously modified using backpropagation till the loss becomes 0.\n",
    "Loss becomes 0 means that the discriminator starts thinking that all the images generated by the generator are real. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the discriminator, we want to push the weights in a direction of its ability to \n",
    "\n",
    "1. detect real images as real \n",
    "2. detect fake images as fake. \n",
    "\n",
    "so both real X and fake X are passed to the discriminator.\n",
    "We receive fake Y and real Y from passing real X and fake X through the discriminator.\n",
    "\n",
    "Now here discriminator should have predicted that the fake image is fake and the real image is real. \n",
    "\n",
    "so fake Y should be 0\n",
    "and real Y should be 1.\n",
    "\n",
    "so we calculate the loss value by comparing fake Y and 0 -> Push fake Y to 0\n",
    "so we calculate the loss value by comparing real Y and 1 -> Push real Y to 1\n",
    "\n",
    "we receive the loss value and update the discriminator weights using backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two phases, training of discriminator and training of the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is information ?\n",
    "\n",
    "Information is the number of bits required to encode data and transmit it.\n",
    "\n",
    "high probabilty event has less information - less surprise, less information\n",
    "Low probability event has more information. - more suprise hence more information\n",
    "\n",
    "to correctly represent this, the equation is \n",
    "h(x) = -log(P(x)) where h(x) is information.\n",
    "this will give high value for low probability\n",
    "\n",
    "e.g log(0.1) = -1 => h(x) = 1 which is high, probability is low i.e just 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy = the number of bits required to represent a randomly selected event from a probability distribution.\n",
    "\n",
    "Skewed distribution has low entropy. (low chances of different result every time occuring so low entropy.)\n",
    "Uniform distribution has high entropy. (more chances of different result every time occuring so high entropy)\n",
    "\n",
    "H(X) = -sum(P(x(i)) * Log (P (x(i))))\n",
    "\n",
    "(sum of probabilty * log of probability) for all samples. \n",
    "\n",
    "skewed distribution has low entropy - it is unsurprising. (mostly we know what we will get in result)\n",
    "unfiorm distribution has high entropy - it is surprising.  (unlikely, what we think is what we might get)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy - the number of bits required to represent an average event from one distribution compared to another. \n",
    "P = target distribution\n",
    "Q = approximation of P.\n",
    "\n",
    "P and Q are the two distributions. \n",
    "\n",
    "Cross entropy is the number of extra bits required to represent an event using Q instead of P.\n",
    "\n",
    "H(P,Q) = -Sum(P(x(i) *  log(Q (x(i)))))\n",
    "\n",
    "How does this apply to our machine learning ?\n",
    "So P is the ideal probability distribution we want to get.\n",
    "Q is the one that we get as an output when training the model. \n",
    "\n",
    "Cross Entropy will be the extra bits for an event between Q and P.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Binary Cross entropy to calculate the discriminator loss.\n",
    "\n",
    "\n",
    "standard equation\n",
    "\n",
    "negative of sum from 0 to n (y * log (y hat) +(1 - y) * log (y hat) ) divided by n.\n",
    "\n",
    "why this equation we have already studied in the past that y is becoming 0 then first part will disappear while if y is becoming 1 then second part will dissapear.\n",
    "\n",
    "log(y hat) is log(D(x^2)) + log (1 - D(G(x^2)))\n",
    "\n",
    "first part is for real, as we know. As y becomes 1, then only it will exist else it will disappear. \n",
    "\n",
    "second part is for fake. \n",
    "and fake image is coming from generator that is why D(G(x^2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also called as the minxmax game, where the discriminator is trying to minimize the equation while the generator is trying to maximize the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator Loss\n",
    "\n",
    "Again applying the Binary Crossentropy function.\n",
    "\n",
    "label = 1, which means image is real. \n",
    "WHy do we want to use label = 1, because as we learnt above for generator if value is not 1, then it will be a loss. For e.g - if it is 0.2 then loss will be high.\n",
    "\n",
    "eqn = average(sum( log (D(G(z^2)))))\n",
    "\n",
    "so log 1 = 0, loss for real image will be 0, which means no more required to fine tune the generator.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generator is trying to push as much as possible the above equation to zero or towards loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
